{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2b9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8223d01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/u/1/uc?export=download&confirm=k3T5&id=1_AckYkinAnhqmRQtGsQgUKAnTHxxX5J0\n",
      "To: /home/ubuntu/RATransformers/notebooks/spider.zip\n",
      "100%|██████████| 99.7M/99.7M [00:01<00:00, 94.9MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'spider.zip'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "spider_url = 'https://drive.google.com/u/1/uc?export=download&confirm=k3T5&id=1_AckYkinAnhqmRQtGsQgUKAnTHxxX5J0'\n",
    "output = 'spider.zip'\n",
    "gdown.download(spider_url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4faf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o spider.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2ae14d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('spider/tables.json') as fp:\n",
    "    tables = {t['db_id']: t for t in json.load(fp)}\n",
    "\n",
    "with open('spider/train_spider.json') as fp:\n",
    "    train_data = json.load(fp)\n",
    "\n",
    "with open('spider/train_others.json') as fp:\n",
    "    train_data += json.load(fp)\n",
    "\n",
    "with open('spider/dev.json') as fp:\n",
    "    test_data = json.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8242d534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 8577  Skipped 82 samples with too long input.\n",
      "Test: 1034  Skipped 0 samples with too long input.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_processed_data(raw_data):\n",
    "    X, y, X_word_relations = [], [], []\n",
    "    n_skip = 0\n",
    "    for d in raw_data:\n",
    "        input_text = d['question'] + f\" | {d['db_id']}\"\n",
    "        \n",
    "        word_relations = defaultdict(dict)\n",
    "\n",
    "        table_span, table_i = None, None\n",
    "        for i, c_name in tables[d['db_id']]['column_names_original']:\n",
    "            if i < 0: continue\n",
    "            if table_i != i:\n",
    "                table_i = i\n",
    "                table_span = (len(input_text + ' | '), len(input_text + ' | ') + len(tables[d['db_id']]['table_names_original'][i]))\n",
    "                input_text += f\" | {tables[d['db_id']]['table_names_original'][i]} : \"\n",
    "\n",
    "                c_span = (len(input_text), len(input_text) + len(c_name))\n",
    "                input_text += c_name\n",
    "\n",
    "            else:\n",
    "                c_span = (len(input_text + ', '), len(input_text + ', ') + len(c_name))\n",
    "                input_text += f', {c_name}'\n",
    "\n",
    "            word_relations[table_span][c_span] = 'table_column_link'\n",
    "            word_relations[c_span][table_span] = 'column_table_link'\n",
    "\n",
    "        if len(input_text.split()) > 200:\n",
    "            # Skipped sample with too long input\n",
    "            n_skip += 1\n",
    "            continue\n",
    "        \n",
    "        X.append(input_text.lower())\n",
    "        y.append((d['db_id'] + ' | ' + d['query']).lower())\n",
    "        X_word_relations.append(word_relations)\n",
    "        \n",
    "    return X, y, X_word_relations, n_skip\n",
    "\n",
    "train_X, train_y, train_X_word_relations, n_skip = get_processed_data(train_data)\n",
    "print(\"Train:\", len(train_X), f\" Skipped {n_skip} samples with too long input.\")\n",
    "test_X, test_y, test_X_word_relations, n_skip = get_processed_data(test_data)\n",
    "print(\"Test:\", len(test_X), f\" Skipped {n_skip} samples with too long input.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68bc75d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "import ratransformers\n",
    "\n",
    "ratransformer = ratransformers.RATransformer(\n",
    "    'tscholak/1zha5ono', \n",
    "    relation_kinds=['table_column_link', 'column_table_link'],\n",
    "    model_cls=AutoModelForSeq2SeqLM\n",
    ")\n",
    "model = ratransformer.model\n",
    "tokenizer = ratransformer.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acf8577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Text2SQLDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, tokenizer, X_word_relations=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_word_relations = X_word_relations or [None] * len(X)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index: int) -> dict:\n",
    "        \n",
    "        source = self.tokenizer(self.X[index], padding='max_length', input_relations=self.X_word_relations[index], return_tensors=\"pt\")\n",
    "        target = self.tokenizer(self.y[index], padding='max_length', input_relations=None, return_tensors=\"pt\")\n",
    "        \n",
    "        source_ids = source[\"input_ids\"].squeeze()\n",
    "        source_input_relations = source[\"input_relations\"].squeeze()\n",
    "        target_ids = target[\"input_ids\"].squeeze()\n",
    "        target_ids[target_ids == 0] = -100\n",
    "\n",
    "        src_mask = source[\"attention_mask\"].squeeze()\n",
    "        target_mask = target[\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": source_ids,\n",
    "            \"attention_mask\": src_mask,\n",
    "            \"label\": target_ids,\n",
    "            \"decoder_attention_mask\": target_mask,\n",
    "            'input_relations': source_input_relations\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "# Get datasets with word relations\n",
    "train_d = Text2SQLDataset(train_X, train_y, tokenizer, train_X_word_relations)\n",
    "val_d = Text2SQLDataset(test_X, test_y, tokenizer, test_X_word_relations)\n",
    "\n",
    "# Get datasets without word relations\n",
    "train_d_without_relations = Text2SQLDataset(train_X, train_y, tokenizer)\n",
    "val_d_without_relations = Text2SQLDataset(test_X, test_y, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fe8de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "# Set training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='checkpoints',\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy='steps',\n",
    "    max_steps=100000,\n",
    "    eval_steps=1000,\n",
    "    seed=42,\n",
    "    save_total_limit=1,\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36c0684f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='518' max='259' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [259/259 1:50:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0085068941116333,\n",
       " 'eval_runtime': 1324.2969,\n",
       " 'eval_samples_per_second': 0.781}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,      \n",
    "    args=training_args,\n",
    "    train_dataset=train_d,         \n",
    "    eval_dataset=val_d,            \n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback()]\n",
    ")\n",
    "\n",
    "# get performance before training\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23b5a658",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='100000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2000/100000 2:56:39 < 144:24:58, 0.19 it/s, Epoch 0/24]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.099700</td>\n",
       "      <td>0.325624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>0.348871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=0.10654444885253907, metrics={'train_runtime': 10601.4604, 'train_samples_per_second': 9.433, 'total_flos': 0, 'epoch': 0.47})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train until early stopping\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06da266f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='259' max='259' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [259/259 20:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.32562369108200073,\n",
       " 'eval_runtime': 1206.6218,\n",
       " 'eval_samples_per_second': 0.857,\n",
       " 'epoch': 0.47}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get performance after training\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9854bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trainer.save_model('ra-tscholak/1zha5ono')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48673297",
   "metadata": {},
   "source": [
    "Training done! After saving, you can then reload the model with the ratransformers package again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c6251b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='259' max='259' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [259/259 15:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.32562369108200073,\n",
       " 'eval_runtime': 938.7328,\n",
       " 'eval_samples_per_second': 1.101}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload model again\n",
    "ratransformer = ratransformers.RATransformer(\n",
    "    'ra-tscholak/1zha5ono', \n",
    "    relation_kinds=['table_column_link', 'column_table_link'],\n",
    "    alias_model_name='t5'\n",
    ")\n",
    "model = ratransformer.model\n",
    "tokenizer = ratransformer.tokenizer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,      \n",
    "    args=training_args,\n",
    "    train_dataset=train_d,         \n",
    "    eval_dataset=val_d,            \n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback()]\n",
    ")\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0202ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
